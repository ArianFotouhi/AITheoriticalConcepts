# Self-Attention and Transformers
In spite of many useful resources for illustrating attention concept, this article tries to recap the concept and more importantly, it shows a simplified numberic example to clarify the process of text generation and translation machine. 
